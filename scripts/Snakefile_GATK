# Snakemake workflow for GATK Best Practices Germline Short Variant Discovery
# WES (Whole Exome Sequencing) Analysis Pipeline
# From raw FASTQ files to annotated VCF files

import os
import pandas as pd
from snakemake.utils import min_version

# Minimum Snakemake version
min_version("7.0")

# --- Load Configuration ---
configfile: "config/config.yaml"

# --- Global Variables ---
SAMPLES = config["samples"]
RESULTS_DIR = config["results_dir"] + "/gatk"

# --- Input Validation ---
def validate_samples():
    """Validate that all required input files exist"""
    missing_files = []
    for sample in SAMPLES:
        r1 = os.path.join(config["data_dir"], f"{sample}_R1.fastq.gz")
        r2 = os.path.join(config["data_dir"], f"{sample}_R2.fastq.gz")
        if not os.path.exists(r1):
            missing_files.append(r1)
        if not os.path.exists(r2):
            missing_files.append(r2)

    if missing_files:
        raise FileNotFoundError(f"Missing input files: {missing_files}")

# Validate samples at workflow start
validate_samples()

# --- Helper Functions ---
def get_fastqs(wildcards):
    """Return FASTQ files for a given sample"""
    return {
        "r1": os.path.join(config["data_dir"], f"{wildcards.sample}_R1.fastq.gz"),
        "r2": os.path.join(config["data_dir"], f"{wildcards.sample}_R2.fastq.gz")
    }

def get_read_group(wildcards):
    """Generate read group string for BWA"""
    rg_config = config["params"]["rg"]
    return f"'@RG\\tID:{rg_config['platform_unit_prefix']}.{wildcards.sample}\\tSM:{wildcards.sample}\\tPL:{rg_config['platform']}\\tLB:{rg_config['library_prefix']}{wildcards.sample}\\tCN:SequencingCenter\\tDS:WES'"

# --- Target Rules ---
rule all:
    input:
        # Final annotated cohort VCF
        os.path.join(RESULTS_DIR, "cohort", f"{config.get('cohort_name', 'cohort')}.annotated.vcf.gz"),
        # MultiQC report
        os.path.join(RESULTS_DIR, "multiqc", "multiqc_report.html"),
        # Individual sample reports
        expand(os.path.join(RESULTS_DIR, "{sample}", "{sample}_summary.txt"), sample=SAMPLES)

# --- Quality Control Rules ---
rule fastqc_raw:
    """Run FastQC on raw FASTQ files"""
    input:
        unpack(get_fastqs)
    output:
        html_r1=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_R1_fastqc.html"),
        zip_r1=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_R1_fastqc.zip"),
        html_r2=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_R2_fastqc.html"),
        zip_r2=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_R2_fastqc.zip")
    params:
        outdir=lambda wildcards, output: os.path.dirname(output.html_r1),
        threads=config["params"].get("fastqc", {}).get("threads", 2)
    log:
        os.path.join(RESULTS_DIR, "logs", "fastqc", "{sample}.log")
    threads: config["params"].get("fastqc", {}).get("threads", 2)
    conda:
        "../environment.yml"
    shell:
        """
        mkdir -p {params.outdir}
        fastqc -o {params.outdir} -t {params.threads} {input.r1} {input.r2} > {log} 2>&1
        """

# --- Alignment Rules ---
rule bwa_mem_sort:
    """Align reads with BWA-MEM and sort with Samtools"""
    input:
        r1=os.path.join(config["data_dir"], "{sample}_R1.fastq.gz"),
        r2=os.path.join(config["data_dir"], "{sample}_R2.fastq.gz"),
        ref=config["reference_genome"],
        ref_idx=config["reference_genome"] + ".bwt"
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam"),
        bai=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam.bai")
    params:
        rg=get_read_group,
        sort_threads=config["params"]["bwa"].get("mem_sort_threads", 4)
    log:
        os.path.join(RESULTS_DIR, "logs", "bwa_mem", "{sample}.log")
    threads: config["params"]["bwa"]["threads"]
    conda:
        "../environment.yml"
    shell:
        """
        (bwa mem -t {threads} -R {params.rg} {input.ref} {input.r1} {input.r2} | \\
         samtools sort -@ {params.sort_threads} -o {output.bam} -) > {log} 2>&1
        samtools index {output.bam}
        """

# --- Preprocessing Rules ---
rule mark_duplicates:
    """Mark duplicate reads with Picard"""
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam"),
        bai=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam.bai")
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        bai=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam.bai"),
        metrics=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.dup_metrics.txt")
    params:
        java_opts=config["params"]["java_opts"]
    log:
        os.path.join(RESULTS_DIR, "logs", "mark_duplicates", "{sample}.log")
    conda:
        "../environment.yml"
    shell:
        """
        gatk --java-options '{params.java_opts}' MarkDuplicates \\
            -I {input.bam} \\
            -O {output.bam} \\
            -M {output.metrics} \\
            --CREATE_INDEX true \\
            --VALIDATION_STRINGENCY SILENT > {log} 2>&1
        """

rule base_recalibrator:
    """Build base quality recalibration model"""
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        bai=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam.bai"),
        ref=config["reference_genome"],
        dbsnp=config["dbsnp"],
        mills=config["mills_indels"],
        targets=config["exome_targets"]
    output:
        table=os.path.join(RESULTS_DIR, "recal", "{sample}.recal_data.table")
    params:
        java_opts=config["params"]["java_opts"],
        threads=config["params"]["gatk"].get("base_recalibrator_threads", 4)
    log:
        os.path.join(RESULTS_DIR, "logs", "base_recalibrator", "{sample}.log")
    conda:
        "../environment.yml"
    shell:
        """
        gatk --java-options '{params.java_opts}' BaseRecalibrator \\
            -R {input.ref} \\
            -I {input.bam} \\
            --known-sites {input.dbsnp} \\
            --known-sites {input.mills} \\
            -L {input.targets} \\
            -O {output.table} \\
            --native-pair-hmm-threads {params.threads} > {log} 2>&1
        """

rule apply_bqsr:
    """Apply base quality score recalibration"""
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        table=os.path.join(RESULTS_DIR, "recal", "{sample}.recal_data.table"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam"),
        bai=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam.bai")
    params:
        java_opts=config["params"]["java_opts"]
    log:
        os.path.join(RESULTS_DIR, "logs", "apply_bqsr", "{sample}.log")
    conda:
        "../environment.yml"
    shell:
        """
        gatk --java-options '{params.java_opts}' ApplyBQSR \\
            -R {input.ref} \\
            -I {input.bam} \\
            -bqsr {input.table} \\
            -L {input.targets} \\
            -O {output.bam} > {log} 2>&1
        """

# --- Variant Calling Rules ---
rule haplotype_caller:
    """Call variants with HaplotypeCaller in GVCF mode"""
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam"),
        bai=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam.bai"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        gvcf=os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz"),
        idx=os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz.tbi")
    params:
        java_opts=config["params"]["java_opts"],
        threads=config["params"]["gatk"].get("haplotype_caller_threads", 4)
    log:
        os.path.join(RESULTS_DIR, "logs", "haplotype_caller", "{sample}.log")
    conda:
        "../environment.yml"
    shell:
        """
        gatk --java-options '{params.java_opts}' HaplotypeCaller \\
            -R {input.ref} \\
            -I {input.bam} \\
            -O {output.gvcf} \\
            -L {input.targets} \\
            -ERC GVCF \\
            --native-pair-hmm-threads {params.threads} > {log} 2>&1
        """

# --- Quality Metrics Rules ---
rule collect_hs_metrics:
    """Collect hybrid selection metrics"""
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        metrics=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.hs_metrics.txt")
    params:
        java_opts=config["params"]["java_opts"]
    log:
        os.path.join(RESULTS_DIR, "logs", "hs_metrics", "{sample}.log")
    conda:
        "../environment.yml"
    shell:
        """
        gatk --java-options '{params.java_opts}' CollectHsMetrics \\
            -I {input.bam} \\
            -O {output.metrics} \\
            -R {input.ref} \\
            -BAIT_INTERVALS {input.targets} \\
            -TARGET_INTERVALS {input.targets} > {log} 2>&1
        """

rule sample_summary:
    """Generate per-sample summary report"""
    input:
        gvcf=os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz"),
        hs_metrics=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.hs_metrics.txt"),
        dup_metrics=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.dup_metrics.txt")
    output:
        summary=os.path.join(RESULTS_DIR, "{sample}", "{sample}_summary.txt")
    shell:
        """
        cat > {output.summary} << 'EOF'
GATK WES Pipeline - Sample Summary Report
=========================================
Sample: {wildcards.sample}
Analysis Date: $(date)
Pipeline: Snakemake GATK Best Practices

Output Files:
- gVCF: {input.gvcf}
- Hybrid Selection Metrics: {input.hs_metrics}
- Duplicate Metrics: {input.dup_metrics}

Quality Control:
- FastQC reports available in results/gatk/qc/{wildcards.sample}/
- All processing logs available in results/gatk/logs/

Next Steps:
1. Review quality metrics
2. Proceed with joint genotyping for cohort analysis
3. Perform variant annotation and filtering
EOF
        """

# --- Joint Genotyping Rules ---
rule create_sample_map:
    """Create sample map for GenomicsDBImport"""
    input:
        gvcfs=expand(os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz"), sample=SAMPLES)
    output:
        sample_map=os.path.join(RESULTS_DIR, "cohort", "sample_map.txt")
    run:
        with open(output.sample_map, 'w') as f:
            for sample in SAMPLES:
                gvcf_path = os.path.join(RESULTS_DIR, "gvcf", f"{sample}.g.vcf.gz")
                f.write(f"{sample}\\t{gvcf_path}\\n")

rule genomicsdb_import:
    """Import gVCFs into GenomicsDB"""
    input:
        gvcfs=expand(os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz"), sample=SAMPLES),
        sample_map=os.path.join(RESULTS_DIR, "cohort", "sample_map.txt"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        db=directory(os.path.join(RESULTS_DIR, "cohort", "cohort_db"))
    params:
        java_opts=config["params"]["java_opts"]
    log:
        os.path.join(RESULTS_DIR, "logs", "genomicsdb_import.log")
    conda:
        "../environment.yml"
    shell:
        """
        gatk --java-options '{params.java_opts}' GenomicsDBImport \\
            --sample-name-map {input.sample_map} \\
            --genomicsdb-workspace-path {output.db} \\
            -L {input.targets} \\
            --reader-threads 5 \\
            --batch-size 50 > {log} 2>&1
        """

rule genotype_gvcfs:
    """Perform joint genotyping"""
    input:
        db=os.path.join(RESULTS_DIR, "cohort", "cohort_db"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        vcf=os.path.join(RESULTS_DIR, "cohort", f"{config.get('cohort_name', 'cohort')}.vcf.gz"),
        idx=os.path.join(RESULTS_DIR, "cohort", f"{config.get('cohort_name', 'cohort')}.vcf.gz.tbi")
    params:
        java_opts=config["params"]["java_opts"],
        threads=config["params"]["gatk"].get("joint_genotyping_threads", 8)
    log:
        os.path.join(RESULTS_DIR, "logs", "genotype_gvcfs.log")
    conda:
        "../environment.yml"
    shell:
        """
        gatk --java-options '{params.java_opts}' GenotypeGVCFs \\
            -R {input.ref} \\
            -V gendb://{input.db} \\
            -O {output.vcf} \\
            -L {input.targets} \\
            --include-non-variant-sites false \\
            --native-pair-hmm-threads {params.threads} > {log} 2>&1
        """

# --- Annotation Rules ---
rule snpeff_annotate:
    """Annotate variants with SnpEff"""
    input:
        vcf=os.path.join(RESULTS_DIR, "cohort", f"{config.get('cohort_name', 'cohort')}.vcf.gz")
    output:
        vcf=os.path.join(RESULTS_DIR, "cohort", f"{config.get('cohort_name', 'cohort')}.annotated.vcf.gz"),
        html=os.path.join(RESULTS_DIR, "cohort", "snpEff_summary.html"),
        csv=os.path.join(RESULTS_DIR, "cohort", "snpEff_genes.txt")
    params:
        genome_db=config["params"]["snpeff"]["genome_db"],
        memory=config["params"]["snpeff"]["memory"]
    log:
        os.path.join(RESULTS_DIR, "logs", "snpeff.log")
    conda:
        "../environment.yml"
    shell:
        """
        snpEff -Xmx{params.memory} -v {params.genome_db} \\
            -stats {output.html} \\
            -csvStats {output.csv} \\
            {input.vcf} | \\
        bgzip -c > {output.vcf} 2> {log}

        tabix -p vcf {output.vcf}
        """

# --- Reporting Rules ---
rule multiqc:
    """Generate MultiQC report"""
    input:
        fastqc=expand(os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_{read}_fastqc.zip"), sample=SAMPLES, read=["R1", "R2"]),
        dup_metrics=expand(os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.dup_metrics.txt"), sample=SAMPLES),
        hs_metrics=expand(os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.hs_metrics.txt"), sample=SAMPLES)
    output:
        report=os.path.join(RESULTS_DIR, "multiqc", "multiqc_report.html")
    params:
        outdir=os.path.join(RESULTS_DIR, "multiqc"),
        title="GATK WES Pipeline Results"
    log:
        os.path.join(RESULTS_DIR, "logs", "multiqc.log")
    conda:
        "../environment.yml"
    shell:
        """
        multiqc \\
            --title '{params.title}' \\
            --filename multiqc_report.html \\
            --outdir {params.outdir} \\
            --force \\
            {RESULTS_DIR} > {log} 2>&1
        """

# --- Cleanup Rules ---
rule clean_intermediate:
    """Clean intermediate files to save space"""
    input:
        final_vcf=os.path.join(RESULTS_DIR, "cohort", f"{config.get('cohort_name', 'cohort')}.annotated.vcf.gz"),
        multiqc=os.path.join(RESULTS_DIR, "multiqc", "multiqc_report.html")
    shell:
        """
        if [ "{config[output][keep_intermediate]}" = "false" ]; then
            echo "Cleaning intermediate files..."
            find {RESULTS_DIR} -name "*.sorted.bam*" -delete
            find {RESULTS_DIR} -name "*marked_dups.bam*" -delete
            echo "Intermediate files cleaned"
        else
            echo "Keeping intermediate files as requested"
        fi
        """