Architecting Reproducible WES Analysis Workflows: A Comparative Implementation of GATK and DeepVariant PipelinesIntroduction: Architecting Reproducible WES Analysis WorkflowsWhole Exome Sequencing (WES) has become a cornerstone of modern genetics, enabling researchers and clinicians to efficiently interrogate the protein-coding regions of the genome. The analytical journey from raw sequencing reads to a set of biologically meaningful variants, however, is a complex, multi-stage process that demands robust, reproducible, and well-understood bioinformatic pipelines.1 A typical WES workflow encompasses several critical phases: raw data quality control, pre-processing of reads, alignment to a reference genome, post-alignment refinement, variant calling, and finally, variant annotation to impute functional significance.1The variant calling step, in particular, represents a critical juncture where different algorithmic philosophies can be employed. This report presents a comprehensive, expert-level guide to constructing two distinct, end-to-end WES analysis pipelines, each centered on a leading variant calling paradigm:The Genome Analysis Toolkit (GATK): Developed at the Broad Institute, GATK represents the long-standing industry standard for germline variant discovery. Its methodology is rooted in sophisticated statistical models, local de-novo assembly of haplotypes, and rigorous data recalibration to produce highly accurate and reliable variant calls.2Google DeepVariant: A modern challenger to the established paradigm, DeepVariant reframes variant calling as an image classification problem. It utilizes a deep convolutional neural network to identify genetic variants from pileup images of sequencing data, an approach that has demonstrated exceptional accuracy, particularly for challenging indel variants.4To provide maximum utility and address different operational needs, each of these two pipelines is implemented in two distinct formats. The first is a self-contained Bash script, which offers a clear, linear, and didactic demonstration of the command sequence, suitable for smaller-scale projects or educational purposes.5 The second is a Snakemake workflow, a powerful, Python-based system that ensures scalability, portability, and unparalleled reproducibility for production-level research and clinical environments.6 This dual-format presentation provides both a conceptual blueprint and a production-ready implementation for comprehensive WES analysis.Section 1: Foundational Components and PrerequisitesA successful and reproducible bioinformatics analysis is built upon a well-organized foundation. Before executing any pipeline, it is imperative to establish a consistent directory structure, prepare the necessary reference data, and manage software dependencies meticulously. This section details the common setup required for both the GATK and DeepVariant workflows.1.1 Environment and Dependency Management with CondaBioinformatics pipelines often rely on a multitude of software tools, each with specific version requirements. Managing these dependencies manually can lead to conflicts and severely compromise the reproducibility of an analysis. The use of a package and environment manager like Conda is therefore considered a best practice.8 By creating an isolated environment, all required software can be installed at specific versions without interfering with other projects or system-level installations.Modern workflow managers, such as Snakemake, have taken this principle a step further by integrating Conda directly. This allows software dependencies to be defined declaratively on a per-rule basis within the workflow itself, creating a self-contained and portable analysis package that can be executed on any system with Conda installed.6 This evolution from manual installation to declarative, integrated dependency management represents a significant advancement in ensuring computational reproducibility.For the pipelines in this report, all necessary tools can be installed into a single Conda environment using the following environment.yml file.environment.yml:YAMLname: wes-analysis
channels:
  - conda-forge
  - bioconda
  - defaults
dependencies:
  - fastqc=0.12.1
  - multiqc=1.24
  - bwa=0.7.17
  - samtools=1.19.2
  - picard=3.1.1
  - gatk4=4.5.0.0
  - snpeff=5.2
  - snakemake-minimal=8.14.0
  # Note: DeepVariant is best run via its official Docker/Singularity container
  # and is not included in this central environment file.
  # It will be handled by Snakemake's container support.
To create and activate this environment, execute the following commands:Bashconda env create -f environment.yml
conda activate wes-analysis
1.2 Reference Data PreparationBoth pipelines require a set of reference genome files. The following script automates the download of the human reference genome (GRCh38) and the known variant sites files required by GATK, and then creates the necessary indices for all tools. These reference files are often made available through public cloud storage buckets.10prepare_reference.sh:Bash#!/bin/bash
# This script downloads the GRCh38 reference genome and related resources,
# then prepares all necessary indices for BWA, Samtools, and GATK.

set -e # Exit immediately if a command exits with a non-zero status.

# --- Configuration ---
REF_DIR="reference"
GENOME_FASTA="GRCh38.p13.genome.fa"
DBSNP_VCF="dbsnp_146.hg38.vcf.gz"
MILLS_INDELS_VCF="Mills_and_1000G_gold_standard.indels.hg38.vcf.gz"

# --- Create Directory ---
mkdir -p "${REF_DIR}"
cd "${REF_DIR}"

# --- Download Files ---
echo ">>> Downloading GRCh38 reference genome..."
wget -c ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GCA_000001405.28_GRCh38.p13_genomic.fna.gz -O ${GENOME_FASTA}.gz
gunzip ${GENOME_FASTA}.gz

echo ">>> Downloading dbSNP VCF for GATK BQSR..."
wget -c https://storage.googleapis.com/gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.gz -O ${DBSNP_VCF}
wget -c https://storage.googleapis.com/gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.gz.tbi -O ${DBSNP_VCF}.tbi

echo ">>> Downloading Mills and 1000G Gold Standard Indels for GATK BQSR..."
wget -c https://storage.googleapis.com/gcp-public-data--broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz -O ${MILLS_INDELS_VCF}
wget -c https://storage.googleapis.com/gcp-public-data--broad-references/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi -O ${MILLS_INDELS_VCF}.tbi

# --- Create Indices ---
echo ">>> Creating BWA index..."
bwa index "${GENOME_FASTA}"

echo ">>> Creating Samtools FASTA index..."
samtools faidx "${GENOME_FASTA}"

echo ">>> Creating GATK Sequence Dictionary..."
gatk CreateSequenceDictionary -R "${GENOME_FASTA}" -O "${GENOME_FASTA%.fa}.dict"

echo ">>> Reference preparation complete."
1.3 Input Data and Directory StructureA standardized directory structure is crucial for the provided scripts to function correctly and for maintaining project organization. The following structure is assumed:wes_analysis/
├── data/
│   ├── sample1_R1.fastq.gz
│   ├── sample1_R2.fastq.gz
│   ├── sample2_R1.fastq.gz
│   └── sample2_R2.fastq.gz
├── reference/
│   ├── GRCh38.p13.genome.fa
│   ├──... (and all index files)
│   └── exome_targets.bed  # User-provided exome coordinates
├── results/
│   ├── gatk/
│   └── deepvariant/
└── scripts/
    ├── gatk_pipeline.sh
    ├── gatk_Snakefile
    ├── deepvariant_pipeline.sh
    └── deepvariant_Snakefile
1.4 The Exome Target Interval FileWES analysis focuses specifically on the captured exon regions, which constitute roughly 1-2% of the genome. An interval file, typically in BED format, defines the genomic coordinates of these target regions. This file is essential for several reasons:It allows variant calling tools to restrict their analysis to the regions of interest, significantly reducing computational time.It is used in quality control metrics (e.g., with Picard's CollectHsMetrics) to assess capture efficiency, on-target rate, and coverage uniformity.2It ensures that downstream analysis is focused on the data most relevant to the experimental design.This file is experiment-specific, depending on the exome capture kit used. The user must provide this file (e.g., exome_targets.bed) and its path will be passed as a critical parameter to the analysis pipelines.Table 1: Required Software and DependenciesTool NameRecommended VersionCore FunctionConda Installation CommandFastQC0.12.1Raw read quality controlconda install -c bioconda fastqc=0.12.1MultiQC1.24Aggregate QC report generationconda install -c bioconda multiqc=1.24BWA0.7.17Read alignment (Burrows-Wheeler Aligner)conda install -c bioconda bwa=0.7.17Samtools1.19.2BAM/SAM file manipulation and processingconda install -c bioconda samtools=1.19.2Picard3.1.1BAM/SAM file manipulation (e.g., MarkDuplicates)conda install -c bioconda picard=3.1.1GATK44.5.0.0Variant discovery and genotyping toolkitconda install -c bioconda gatk4=4.5.0.0DeepVariant1.6.1Deep learning-based variant callerRun via official Docker/Singularity containerSnpEff5.2Variant annotation and effect predictionconda install -c bioconda snpeff=5.2Snakemake8.14.0Workflow management systemconda install -c conda-forge -c bioconda snakemake-minimal=8.14.0Section 2: The GATK Best Practices Pipeline for Germline Short VariantsThis section details the implementation of a WES analysis pipeline adhering to the GATK Best Practices for Germline Short Variant Discovery.3 This workflow is the result of extensive validation and optimization at the Broad Institute and is widely considered the benchmark for germline analysis.2.1 Conceptual FrameworkThe GATK4 workflow is architected to be both highly accurate and scalable to massive cohorts. Its design directly addresses the logistical and computational challenges encountered in large-scale sequencing projects.Data Pre-processing: The initial steps transform raw sequencing data into an "analysis-ready" BAM file.13 This involves:Alignment: Reads are mapped to the reference genome using BWA-MEM, an aligner well-suited for Illumina sequencing data.14Sorting: The resulting alignments are sorted by coordinate using Samtools, a prerequisite for most downstream tools.Duplicate Marking: Picard MarkDuplicates identifies and flags PCR duplicates that arise during library preparation. Failing to remove these can lead to false positive variant calls due to the artificial inflation of evidence for sequencing errors.2Base Quality Score Recalibration (BQSR): This two-pass process is a hallmark of the GATK workflow. BaseRecalibrator builds a model of systematic errors in the base quality scores produced by the sequencer, using known polymorphic sites (e.g., dbSNP) as a reference.14ApplyBQSR then uses this model to adjust the quality scores in the BAM file, providing the variant caller with more accurate input data and reducing false positives.13Scalable Variant Calling: Early versions of GATK employed a joint calling strategy that, while powerful, suffered from two major drawbacks: it scaled exponentially with cohort size, and it required a complete re-analysis of all samples whenever a new sample was added (the "N+1 problem").17 The modern GATK4 architecture solves this with a decoupled, two-stage process 18:Per-Sample Calling (HaplotypeCaller): The computationally intensive HaplotypeCaller tool is run on each sample's analysis-ready BAM file independently. It operates in a special emission mode (-ERC GVCF) to produce a Genomic VCF (gVCF) file. A gVCF contains information not only for sites with evidence of variation but also for homozygous-reference sites, summarized in blocks.18Cohort Joint Genotyping: The lightweight gVCFs from all samples in the cohort are first consolidated into a specialized GenomicsDB datastore using the GenomicsDBImport tool. This step is highly efficient and scalable.18 Finally, GenotypeGVCFs is run on this database to perform joint genotyping across the entire cohort. This step leverages the population-wide information to make more sensitive and accurate genotype calls, especially for low-frequency variants, and produces a final, squared-off VCF matrix ready for downstream analysis.17 This architecture elegantly solves the N+1 problem, as adding a new sample only requires generating its gVCF and re-running the fast consolidation and joint genotyping steps.2.2 Implementation 1: The Bash Script Workflow (gatk_pipeline.sh)This script provides a linear, step-by-step implementation of the GATK Best Practices workflow. It is designed to process a single sample from FASTQ to gVCF, with a final section showing how to perform joint genotyping on a cohort of gVCFs.gatk_pipeline.sh:Bash#!/bin/bash

# This script implements the GATK Best Practices workflow for a single sample,
# from raw FASTQ files to an analysis-ready BAM and a gVCF file.
# It concludes with an example of joint genotyping a cohort.

set -euo pipefail # Exit on error, unset variable, or pipe failure

# --- CONFIGURATION ---
# Paths to reference files and tools. Modify these for your system.
REF_DIR="reference"
REF_GENOME="${REF_DIR}/GRCh38.p13.genome.fa"
DBSNP_VCF="${REF_DIR}/dbsnp_146.hg38.vcf.gz"
MILLS_INDELS_VCF="${REF_DIR}/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz"
EXOME_TARGETS="${REF_DIR}/exome_targets.bed" # IMPORTANT: User must provide this file

# Input files
DATA_DIR="data"
SAMPLE_NAME="sample1" # Change this for each sample
FASTQ_R1="${DATA_DIR}/${SAMPLE_NAME}_R1.fastq.gz"
FASTQ_R2="${DATA_DIR}/${SAMPLE_NAME}_R2.fastq.gz"

# Output directory
RESULTS_DIR="results/gatk/${SAMPLE_NAME}"
mkdir -p "${RESULTS_DIR}"

# Tool settings
THREADS=8
MEM_GB=16
GATK_JAVA_OPTS="-Xmx${MEM_GB}G -Djava.io.tmpdir=./tmp"
mkdir -p./tmp

# Read Group Information (critical for GATK)
RG_ID="group1.${SAMPLE_NAME}"
RG_PL="ILLUMINA"
RG_PU="unit1"
RG_LB="lib1"
RG_SM="${SAMPLE_NAME}"

echo "--- Starting GATK WES Pipeline for sample: ${SAMPLE_NAME} ---"

# --- STEP 1: Alignment (BWA-MEM) and Sorting (Samtools) ---
echo ">>> [1/5] Aligning reads with BWA-MEM and sorting..."
ALIGNED_SORTED_BAM="${RESULTS_DIR}/${SAMPLE_NAME}.sorted.bam"

if; then
    bwa mem -t "${THREADS}" \
        -R "@RG\tID:${RG_ID}\tPL:${RG_PL}\tPU:${RG_PU}\tLB:${RG_LB}\tSM:${RG_SM}" \
        "${REF_GENOME}" \
        "${FASTQ_R1}" \
        "${FASTQ_R2}" | \
    samtools sort -@ "${THREADS}" -o "${ALIGNED_SORTED_BAM}" -
    samtools index "${ALIGNED_SORTED_BAM}"
else
    echo "Sorted BAM already exists. Skipping."
fi

# --- STEP 2: Mark Duplicates (Picard) ---
echo ">>> [2/5] Marking duplicates with Picard..."
MARKED_DUPS_BAM="${RESULTS_DIR}/${SAMPLE_NAME}.marked_dups.bam"
METRICS_FILE="${RESULTS_DIR}/${SAMPLE_NAME}.dup_metrics.txt"

if; then
    gatk --java-options "${GATK_JAVA_OPTS}" MarkDuplicates \
        -I "${ALIGNED_SORTED_BAM}" \
        -O "${MARKED_DUPS_BAM}" \
        -M "${METRICS_FILE}" \
        --CREATE_INDEX true
else
    echo "Marked duplicates BAM already exists. Skipping."
fi

# --- STEP 3: Base Quality Score Recalibration (BQSR) ---
echo ">>> [3/5] Performing Base Quality Score Recalibration (BQSR)..."
RECAL_TABLE="${RESULTS_DIR}/${SAMPLE_NAME}.recal_data.table"
ANALYSIS_READY_BAM="${RESULTS_DIR}/${SAMPLE_NAME}.analysis_ready.bam"

if; then
    # 3a: Build the recalibration model
    gatk --java-options "${GATK_JAVA_OPTS}" BaseRecalibrator \
        -R "${REF_GENOME}" \
        -I "${MARKED_DUPS_BAM}" \
        --known-sites "${DBSNP_VCF}" \
        --known-sites "${MILLS_INDELS_VCF}" \
        -L "${EXOME_TARGETS}" \
        -O "${RECAL_TABLE}"

    # 3b: Apply the model to the BAM file
    gatk --java-options "${GATK_JAVA_OPTS}" ApplyBQSR \
        -R "${REF_GENOME}" \
        -I "${MARKED_DUPS_BAM}" \
        -bqsr "${RECAL_TABLE}" \
        -L "${EXOME_TARGETS}" \
        -O "${ANALYSIS_READY_BAM}"
else
    echo "Analysis-ready BAM already exists. Skipping."
fi

# --- STEP 4: Per-Sample Variant Calling (HaplotypeCaller in GVCF mode) ---
echo ">>> [4/5] Calling variants with HaplotypeCaller in GVCF mode..."
GVCF_FILE="${RESULTS_DIR}/${SAMPLE_NAME}.g.vcf.gz"

if [! -f "${GVCF_FILE}" ]; then
    gatk --java-options "${GATK_JAVA_OPTS}" HaplotypeCaller \
        -R "${REF_GENOME}" \
        -I "${ANALYSIS_READY_BAM}" \
        -O "${GVCF_FILE}" \
        -L "${EXOME_TARGETS}" \
        -ERC GVCF
else
    echo "GVCF file already exists. Skipping."
fi

echo "--- Per-sample processing for ${SAMPLE_NAME} complete. ---"
echo "Output gVCF is at: ${GVCF_FILE}"

# --- STEP 5: Cohort Joint Genotyping (Example for a 3-sample cohort) ---
# This part should be run once all samples have been processed through Step 4.
echo ">>> [5/5] Example for Joint Genotyping..."
COHORT_RESULTS_DIR="results/gatk/cohort"
GENOMICS_DB="--genomicsdb-workspace-path ${COHORT_RESULTS_DIR}/cohort_db"
COHORT_VCF="${COHORT_RESULTS_DIR}/cohort.vcf.gz"

# Create a list of gVCFs to process. In a real scenario, this would be
# generated dynamically.
# gvcf_list=(
#   "results/gatk/sample1/sample1.g.vcf.gz"
#   "results/gatk/sample2/sample2.g.vcf.gz"
#   "results/gatk/sample3/sample3.g.vcf.gz"
# )
#
# # Check if the final cohort VCF exists before running
# if; then
#   mkdir -p "${COHORT_RESULTS_DIR}"
#
#   # 5a: Consolidate gVCFs
#   gatk --java-options "${GATK_JAVA_OPTS}" GenomicsDBImport \
#     -R "${REF_GENOME}" \
#     $(for gvcf in "${gvcf_list[@]}"; do echo "-V ${gvcf}"; done) \
#     "${GENOMICS_DB}" \
#     -L "${EXOME_TARGETS}" \
#     --reader-threads 5
#
#   # 5b: Joint Genotype the cohort
#   gatk --java-options "${GATK_JAVA_OPTS}" GenotypeGVCFs \
#     -R "${REF_GENOME}" \
#     -V "gendb://${COHORT_RESULTS_DIR}/cohort_db" \
#     -O "${COHORT_VCF}" \
#     -L "${EXOME_TARGETS}"
#
#   echo "--- Cohort Joint Genotyping complete. ---"
#   echo "Final cohort VCF is at: ${COHORT_VCF}"
# else
#   echo "Cohort VCF already exists. Skipping joint genotyping."
# fi

echo "--- GATK Pipeline Finished ---"
2.3 Implementation 2: The Snakemake Workflow (gatk_Snakefile)This implementation encapsulates the GATK workflow in a Snakefile, which defines the rules for generating output files from input files. A config.yaml file is used to separate the configuration (paths, parameters) from the pipeline logic, enhancing modularity and reusability.config.yaml:YAML# Configuration file for the GATK WES Snakemake workflow

# List of sample names. Snakemake will find FASTQ files based on these.
samples:
  - sample1
  - sample2
  - sample3

# Paths to input data and reference files
data_dir: "data"
ref_dir: "reference"
results_dir: "results/gatk"

reference_genome: "reference/GRCh38.p13.genome.fa"
dbsnp: "reference/dbsnp_146.hg38.vcf.gz"
mills_indels: "reference/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz"
exome_targets: "reference/exome_targets.bed" # IMPORTANT: User must provide this file

# Parameters for tools
params:
  threads: 8
  mem_gb: 16
  # Read Group info will be inferred from sample names
  rg:
    platform: "ILLUMINA"
    library_prefix: "lib"
    platform_unit_prefix: "unit"
gatk_Snakefile:Python# Snakemake workflow for GATK Best Practices Germline Short Variant Discovery

import os

# --- Load Configuration ---
configfile: "config.yaml"

# --- Define Samples and Final Output Files ---
SAMPLES = config["samples"]
RESULTS_DIR = config["results_dir"]

rule all:
    input:
        # Final cohort VCF is the main target
        os.path.join(RESULTS_DIR, "cohort", "cohort.annotated.vcf.gz"),
        # Also generate a MultiQC report for all samples
        os.path.join(RESULTS_DIR, "multiqc", "multiqc_report.html")

# --- Helper function to get FASTQ paths ---
def get_fastqs(wildcards):
    return {
        "r1": os.path.join(config["data_dir"], f"{wildcards.sample}_R1.fastq.gz"),
        "r2": os.path.join(config["data_dir"], f"{wildcards.sample}_R2.fastq.gz")
    }

# --- Workflow Rules ---

# Rule 1: Raw FASTQ Quality Control with FastQC
rule fastqc_raw:
    input:
        unpack(get_fastqs)
    output:
        html=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_{read}_fastqc.html"),
        zip=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_{read}_fastqc.zip")
    params:
        outdir=lambda wildcards, output: os.path.dirname(output.html)
    log:
        os.path.join(RESULTS_DIR, "logs", "fastqc_raw", "{sample}_{read}.log")
    threads: 1
    conda:
        "environment.yml"
    shell:
        "fastqc -o {params.outdir} -t {threads} {input} > {log} 2>&1"

# Rule 2: Align reads with BWA-MEM and sort with Samtools
rule bwa_mem_sort:
    input:
        r1=os.path.join(config["data_dir"], "{sample}_R1.fastq.gz"),
        r2=os.path.join(config["data_dir"], "{sample}_R2.fastq.gz"),
        ref=config["reference_genome"]
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam"),
        idx=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam.bai")
    params:
        rg=lambda w: f"'@RG\\tID:{config['params']['rg']['platform_unit_prefix']}.{w.sample}\\tSM:{w.sample}\\tPL:{config['params']['rg']['platform']}\\tLB:{config['params']['rg']['library_prefix']}{w.sample}'"
    log:
        os.path.join(RESULTS_DIR, "logs", "bwa_mem", "{sample}.log")
    threads: config["params"]["threads"]
    conda:
        "environment.yml"
    shell:
        "(bwa mem -t {threads} -R {params.rg} {input.ref} {input.r1} {input.r2} | "
        "samtools sort -@ {threads} -o {output.bam} -) > {log} 2>&1 && "
        "samtools index {output.bam}"

# Rule 3: Mark Duplicates with Picard
rule mark_duplicates:
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam"),
        idx=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam.bai")
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        idx=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam.bai"),
        metrics=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.dup_metrics.txt")
    params:
        mem_gb=config["params"]["mem_gb"]
    log:
        os.path.join(RESULTS_DIR, "logs", "mark_duplicates", "{sample}.log")
    conda:
        "environment.yml"
    shell:
        "gatk --java-options '-Xmx{params.mem_gb}G' MarkDuplicates "
        "-I {input.bam} -O {output.bam} -M {output.metrics} "
        "--CREATE_INDEX true > {log} 2>&1"

# Rule 4: Base Quality Score Recalibration (BQSR) - Step 1
rule bqsr_recalibrate:
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        ref=config["reference_genome"],
        dbsnp=config["dbsnp"],
        mills=config["mills_indels"],
        targets=config["exome_targets"]
    output:
        table=os.path.join(RESULTS_DIR, "recal", "{sample}.recal_data.table")
    params:
        mem_gb=config["params"]["mem_gb"]
    log:
        os.path.join(RESULTS_DIR, "logs", "bqsr_recalibrate", "{sample}.log")
    conda:
        "environment.yml"
    shell:
        "gatk --java-options '-Xmx{params.mem_gb}G' BaseRecalibrator "
        "-R {input.ref} -I {input.bam} "
        "--known-sites {input.dbsnp} --known-sites {input.mills} "
        "-L {input.targets} -O {output.table} > {log} 2>&1"

# Rule 5: Apply BQSR - Step 2
rule bqsr_apply:
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        table=os.path.join(RESULTS_DIR, "recal", "{sample}.recal_data.table"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam"),
        idx=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam.bai")
    params:
        mem_gb=config["params"]["mem_gb"]
    log:
        os.path.join(RESULTS_DIR, "logs", "bqsr_apply", "{sample}.log")
    conda:
        "environment.yml"
    shell:
        "gatk --java-options '-Xmx{params.mem_gb}G' ApplyBQSR "
        "-R {input.ref} -I {input.bam} -bqsr {input.table} "
        "-L {input.targets} -O {output.bam} > {log} 2>&1"

# Rule 6: Per-sample variant calling with HaplotypeCaller
rule haplotype_caller:
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.analysis_ready.bam"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        gvcf=os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz")
    params:
        mem_gb=config["params"]["mem_gb"]
    log:
        os.path.join(RESULTS_DIR, "logs", "haplotype_caller", "{sample}.log")
    conda:
        "environment.yml"
    shell:
        "gatk --java-options '-Xmx{params.mem_gb}G' HaplotypeCaller "
        "-R {input.ref} -I {input.bam} -O {output.gvcf} "
        "-L {input.targets} -ERC GVCF > {log} 2>&1"

# Rule 7: Consolidate gVCFs with GenomicsDBImport
rule genomicsdb_import:
    input:
        gvcfs=expand(os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz"), sample=SAMPLES),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        db=directory(os.path.join(RESULTS_DIR, "cohort", "cohort_db"))
    params:
        mem_gb=config["params"]["mem_gb"]
    log:
        os.path.join(RESULTS_DIR, "logs", "genomicsdb_import.log")
    conda:
        "environment.yml"
    shell:
        "gatk --java-options '-Xmx{params.mem_gb}G' GenomicsDBImport "
        "-R {input.ref} "
        "-V {os.path.dirname(input.gvcfs)} " # Pass directory of gVCFs
        "--genomicsdb-workspace-path {output.db} "
        "-L {input.targets} --reader-threads 5 > {log} 2>&1"

# Rule 8: Joint-call cohort with GenotypeGVCFs
rule genotype_gvcfs:
    input:
        db=os.path.join(RESULTS_DIR, "cohort", "cohort_db"),
        ref=config["reference_genome"],
        targets=config["exome_targets"]
    output:
        vcf=os.path.join(RESULTS_DIR, "cohort", "cohort.vcf.gz")
    params:
        mem_gb=config["params"]["mem_gb"]
    log:
        os.path.join(RESULTS_DIR, "logs", "genotype_gvcfs.log")
    conda:
        "environment.yml"
    shell:
        "gatk --java-options '-Xmx{params.mem_gb}G' GenotypeGVCFs "
        "-R {input.ref} -V gendb://{input.db} -O {output.vcf} "
        "-L {input.targets} > {log} 2>&1"

# Rule 9: Annotate final VCF with SnpEff
rule snpeff_annotate:
    input:
        vcf=os.path.join(RESULTS_DIR, "cohort", "cohort.vcf.gz")
    output:
        vcf=os.path.join(RESULTS_DIR, "cohort", "cohort.annotated.vcf.gz"),
        html=os.path.join(RESULTS_DIR, "cohort", "snpEff_summary.html")
    params:
        genome_db="GRCh38.86", # SnpEff database name
        mem_gb=config["params"]["mem_gb"]
    log:
        os.path.join(RESULTS_DIR, "logs", "snpeff.log")
    conda:
        "environment.yml"
    shell:
        "snpEff -Xmx{params.mem_gb}g -v {params.genome_db} {input.vcf} | "
        "bgzip -c > {output.vcf} 2> {log}"

# Rule 10: Aggregate QC results with MultiQC
rule multiqc:
    input:
        expand(os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_R1_fastqc.zip"), sample=SAMPLES),
        expand(os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}_R2_fastqc.zip"), sample=SAMPLES),
        expand(os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.dup_metrics.txt"), sample=SAMPLES)
    output:
        os.path.join(RESULTS_DIR, "multiqc", "multiqc_report.html")
    log:
        os.path.join(RESULTS_DIR, "logs", "multiqc.log")
    conda:
        "environment.yml"
    shell:
        "multiqc -o {os.path.dirname(output)} {input} > {log} 2>&1"
Section 3: The DeepVariant PipelineThis section presents a WES analysis pipeline centered on Google's DeepVariant, a variant caller that leverages deep learning for high-accuracy variant identification. This workflow capitalizes on the robustness of the neural network model to simplify certain aspects of the pipeline compared to GATK.3.1 Conceptual FrameworkDeepVariant transforms the task of variant calling into an image classification problem. For each potential variant site, it generates a multi-channel tensor (an "image") that encodes read alignments, base qualities, and other relevant information from the input BAM file. A pre-trained Convolutional Neural Network (CNN) then classifies this image to determine the genotype (homozygous reference, heterozygous, or homozygous alternate).4This approach has several key implications for the overall pipeline architecture:Shared Pre-processing: The accuracy of DeepVariant, like any variant caller, is highly dependent on the quality of the input alignments. Therefore, the foundational pre-processing steps of alignment with BWA-MEM and duplicate marking with Picard remain essential best practices.19Omission of BQSR: The DeepVariant CNN models are trained on vast quantities of real sequencing data that inherently contain the systematic errors BQSR is designed to correct. The network learns to be robust to these errors, effectively performing its own internal error modeling during classification. Consequently, the BQSR step is not required and should be omitted, simplifying the pre-processing workflow.19Simplified Filtering: DeepVariant's reported genotype qualities (GQs) are well-calibrated, meaning they accurately reflect the probability of a correct genotype call. As a result, complex post-calling filtering steps like GATK's VQSR are unnecessary. A simple threshold on the QUAL field or the FILTER status (which DeepVariant sets to PASS for high-quality calls) is sufficient for most applications.4Joint Calling Strategy: Similar to the GATK workflow, the recommended strategy for cohort analysis with DeepVariant involves a two-step process. First, DeepVariant is run on each sample individually to produce a gVCF file. Second, a specialized third-party tool, GLnexus, is used to merge these gVCFs and perform joint genotyping. GLnexus is specifically optimized to work with DeepVariant's gVCF output and can scale to extremely large cohorts.203.2 Implementation 1: The Bash Script Workflow (deepvariant_pipeline.sh)This script reuses the initial pre-processing steps from the GATK script but replaces the GATK-specific stages with a call to DeepVariant. It leverages the official DeepVariant Docker container, which is the recommended method for deployment as it encapsulates all of the tool's complex dependencies.20deepvariant_pipeline.sh:Bash#!/bin/bash

# This script implements a WES pipeline using DeepVariant for variant calling.
# It processes a single sample from FASTQ to a DeepVariant gVCF.
# It assumes Docker is installed and the user has permission to run it.

set -euo pipefail

# --- CONFIGURATION ---
REF_DIR="reference"
REF_GENOME="${REF_DIR}/GRCh38.p13.genome.fa"
EXOME_TARGETS="${REF_DIR}/exome_targets.bed" # IMPORTANT: User must provide this file

# Input files
DATA_DIR="data"
SAMPLE_NAME="sample1" # Change this for each sample
FASTQ_R1="${DATA_DIR}/${SAMPLE_NAME}_R1.fastq.gz"
FASTQ_R2="${DATA_DIR}/${SAMPLE_NAME}_R2.fastq.gz"

# Output directory
RESULTS_DIR="results/deepvariant/${SAMPLE_NAME}"
mkdir -p "${RESULTS_DIR}"

# Tool settings
THREADS=8
DEEPVARIANT_VERSION="1.6.1" # Use the latest recommended version

# Read Group Information
RG_ID="group1.${SAMPLE_NAME}"
RG_PL="ILLUMINA"
RG_PU="unit1"
RG_LB="lib1"
RG_SM="${SAMPLE_NAME}"

echo "--- Starting DeepVariant WES Pipeline for sample: ${SAMPLE_NAME} ---"

# --- STEP 1: Alignment and Sorting (Same as GATK pipeline) ---
echo ">>> [1/3] Aligning reads with BWA-MEM and sorting..."
ALIGNED_SORTED_BAM="${RESULTS_DIR}/${SAMPLE_NAME}.sorted.bam"

if; then
    bwa mem -t "${THREADS}" \
        -R "@RG\tID:${RG_ID}\tPL:${RG_PL}\tPU:${RG_PU}\tLB:${RG_LB}\tSM:${RG_SM}" \
        "${REF_GENOME}" \
        "${FASTQ_R1}" \
        "${FASTQ_R2}" | \
    samtools sort -@ "${THREADS}" -o "${ALIGNED_SORTED_BAM}" -
    samtools index "${ALIGNED_SORTED_BAM}"
else
    echo "Sorted BAM already exists. Skipping."
fi

# --- STEP 2: Mark Duplicates (Same as GATK pipeline) ---
echo ">>> [2/3] Marking duplicates with Picard..."
MARKED_DUPS_BAM="${RESULTS_DIR}/${SAMPLE_NAME}.marked_dups.bam"
METRICS_FILE="${RESULTS_DIR}/${SAMPLE_NAME}.dup_metrics.txt"

if; then
    # Using Picard directly as GATK4 bundles it
    gatk MarkDuplicates \
        -I "${ALIGNED_SORTED_BAM}" \
        -O "${MARKED_DUPS_BAM}" \
        -M "${METRICS_FILE}" \
        --CREATE_INDEX true
else
    echo "Marked duplicates BAM already exists. Skipping."
fi

# --- STEP 3: Variant Calling with DeepVariant ---
echo ">>> [3/3] Calling variants with DeepVariant..."
OUTPUT_VCF="${RESULTS_DIR}/${SAMPLE_NAME}.vcf.gz"
OUTPUT_GVCF="${RESULTS_DIR}/${SAMPLE_NAME}.g.vcf.gz"

if; then
    # Note: We need to use absolute paths for Docker volume mounts
    BAM_PATH=$(realpath "${MARKED_DUPS_BAM}")
    REF_PATH=$(realpath "${REF_GENOME}")
    TARGETS_PATH=$(realpath "${EXOME_TARGETS}")
    OUT_DIR_PATH=$(realpath "${RESULTS_DIR}")

    docker run \
      -v "${BAM_PATH}":"/input/bam" \
      -v "${REF_PATH}":"/input/ref.fa" \
      -v "${REF_PATH}.fai":"/input/ref.fa.fai" \
      -v "${TARGETS_PATH}":"/input/targets.bed" \
      -v "${OUT_DIR_PATH}":"/output" \
      google/deepvariant:"${DEEPVARIANT_VERSION}" \
      /opt/deepvariant/bin/run_deepvariant \
      --model_type=WES \
      --ref=/input/ref.fa \
      --reads=/input/bam \
      --regions=/input/targets.bed \
      --output_vcf=/output/"${SAMPLE_NAME}".vcf.gz \
      --output_gvcf=/output/"${SAMPLE_NAME}".g.vcf.gz \
      --num_shards="${THREADS}"
else
    echo "DeepVariant GVCF already exists. Skipping."
fi

echo "--- DeepVariant per-sample processing for ${SAMPLE_NAME} complete. ---"
echo "Output gVCF is at: ${OUTPUT_GVCF}"
echo "--- DeepVariant Pipeline Finished ---"
3.3 Implementation 2: The Snakemake Workflow (deepvariant_Snakefile)This Snakemake workflow reuses the pre-processing rules (alignment and duplicate marking) and adds a rule for DeepVariant. It leverages Snakemake's built-in container support to execute DeepVariant in a reproducible manner, and can be easily extended for cohort joint genotyping with GLnexus.config.yaml:YAML# Configuration file for the DeepVariant WES Snakemake workflow

samples:
  - sample1
  - sample2
  - sample3

# Paths
data_dir: "data"
ref_dir: "reference"
results_dir: "results/deepvariant"

reference_genome: "reference/GRCh38.p13.genome.fa"
exome_targets: "reference/exome_targets.bed" # IMPORTANT: User must provide this file

# Parameters
params:
  threads: 8
  deepvariant_version: "1.6.1"
  rg:
    platform: "ILLUMINA"
    library_prefix: "lib"
    platform_unit_prefix: "unit"
deepvariant_Snakefile:Python# Snakemake workflow for WES analysis using DeepVariant

import os

# --- Load Configuration ---
configfile: "config.yaml"

# --- Define Samples and Final Output Files ---
SAMPLES = config["samples"]
RESULTS_DIR = config["results_dir"]

rule all:
    input:
        # The main targets are the annotated gVCFs for each sample
        expand(os.path.join(RESULTS_DIR, "annotated", "{sample}.g.vcf.gz"), sample=SAMPLES)

# --- Pre-processing Rules (can be copied from gatk_Snakefile or put in a common file) ---

rule bwa_mem_sort:
    input:
        r1=os.path.join(config["data_dir"], "{sample}_R1.fastq.gz"),
        r2=os.path.join(config["data_dir"], "{sample}_R2.fastq.gz"),
        ref=config["reference_genome"]
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam"),
        idx=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam.bai")
    params:
        rg=lambda w: f"'@RG\\tID:{config['params']['rg']['platform_unit_prefix']}.{w.sample}\\tSM:{w.sample}\\tPL:{config['params']['rg']['platform']}\\tLB:{config['params']['rg']['library_prefix']}{w.sample}'"
    log:
        os.path.join(RESULTS_DIR, "logs", "bwa_mem", "{sample}.log")
    threads: config["params"]["threads"]
    conda:
        "environment.yml"
    shell:
        "(bwa mem -t {threads} -R {params.rg} {input.ref} {input.r1} {input.r2} | "
        "samtools sort -@ {threads} -o {output.bam} -) > {log} 2>&1 && "
        "samtools index {output.bam}"

rule mark_duplicates:
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam"),
        idx=os.path.join(RESULTS_DIR, "bam", "{sample}.sorted.bam.bai")
    output:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        idx=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam.bai"),
        metrics=os.path.join(RESULTS_DIR, "qc", "{sample}", "{sample}.dup_metrics.txt")
    params:
        mem_gb=16
    log:
        os.path.join(RESULTS_DIR, "logs", "mark_duplicates", "{sample}.log")
    conda:
        "environment.yml"
    shell:
        "gatk --java-options '-Xmx{params.mem_gb}G' MarkDuplicates "
        "-I {input.bam} -O {output.bam} -M {output.metrics} "
        "--CREATE_INDEX true > {log} 2>&1"

# --- Variant Calling Rule ---

rule deepvariant:
    input:
        bam=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam"),
        bai=os.path.join(RESULTS_DIR, "bam", "{sample}.marked_dups.bam.bai"),
        ref=config["reference_genome"],
        ref_fai=config["reference_genome"] + ".fai",
        targets=config["exome_targets"]
    output:
        vcf=os.path.join(RESULTS_DIR, "vcf", "{sample}.vcf.gz"),
        gvcf=os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz")
    params:
        version=config["params"]["deepvariant_version"]
    log:
        os.path.join(RESULTS_DIR, "logs", "deepvariant", "{sample}.log")
    threads: config["params"]["threads"]
    container:
        f"docker://google/deepvariant:{params.version}"
    shell:
        "/opt/deepvariant/bin/run_deepvariant "
        "--model_type=WES "
        "--ref={input.ref} "
        "--reads={input.bam} "
        "--regions={input.targets} "
        "--output_vcf={output.vcf} "
        "--output_gvcf={output.gvcf} "
        "--num_shards={threads} > {log} 2>&1"

# --- Annotation Rule ---
rule snpeff_annotate_deepvariant:
    input:
        gvcf=os.path.join(RESULTS_DIR, "gvcf", "{sample}.g.vcf.gz")
    output:
        vcf=os.path.join(RESULTS_DIR, "annotated", "{sample}.g.vcf.gz")
    params:
        genome_db="GRCh38.86",
        mem_gb=16
    log:
        os.path.join(RESULTS_DIR, "logs", "snpeff", "{sample}.log")
    conda:
        "environment.yml"
    shell:
        "snpEff -Xmx{params.mem_gb}g -v {params.genome_db} {input.gvcf} | "
        "bgzip -c > {output.vcf} 2> {log}"

Section 4: Final Step: Variant Annotation with SnpEffA VCF file produced by a variant caller identifies the genomic locations of variants but provides little biological context. The final, critical step in any WES pipeline is variant annotation, which enriches the VCF with information about the variant's potential functional impact.23 This process determines if a variant falls within a gene, if it alters a protein sequence, and predicts the severity of that alteration.4.1 Tool Selection and RationaleTwo of the most widely used command-line annotation tools are SnpEff and Ensembl's Variant Effect Predictor (VEP).24 While both tools perform the same fundamental task, they can produce discordant annotations, particularly for complex indel variants or in regions with overlapping transcripts.26 The choice of transcript database (e.g., Ensembl, RefSeq) and tool-specific parameters can significantly influence the results.28For the implementations in this report, SnpEff is used. It offers a straightforward command-line interface, comprehensive annotation capabilities, and a simple, self-contained system for managing annotation databases. This makes it easy to integrate into automated workflows.234.2 Integrating SnpEff into the WorkflowsBefore running SnpEff, the appropriate reference genome database must be downloaded. SnpEff manages this with a simple download command. For the GRCh38 reference genome, a compatible database (e.g., GRCh38.86) can be downloaded as follows:Bash# Download the SnpEff database for GRCh38 (Ensembl version 86)
java -jar /path/to/snpEff.jar download -v GRCh38.86
Once the database is available, annotation is performed by running SnpEff on the final VCF file. The command specifies the genome database to use and the input VCF. SnpEff adds its annotations to the INFO field of the VCF file under the ANN tag.30Example SnpEff Command:Bash# Annotate a VCF file, allocating 8GB of memory
java -Xmx8g -jar /path/to/snpEff.jar -v GRCh38.86 cohort.vcf.gz > cohort.annotated.vcf.gz
This annotation step has been incorporated as the final rule in both the GATK and DeepVariant Snakemake workflows presented in the preceding sections, ensuring a fully automated pipeline from raw reads to a functionally annotated VCF file. For the Bash scripts, this command can be appended after the final VCF has been generated.Conclusion: Selecting the Appropriate Pipeline for Your ResearchThis report has detailed the architecture and implementation of two state-of-the-art pipelines for Whole Exome Sequencing analysis, one based on the GATK Best Practices and the other on Google's DeepVariant. Both workflows are capable of producing high-quality variant calls, yet they embody different algorithmic philosophies and present distinct operational trade-offs. The choice between them should be guided by the specific goals, scale, and computational context of the research project.The GATK pipeline remains the established standard, particularly for large-scale cohort studies. Its strength lies in the joint genotyping process, which leverages population-level data to increase sensitivity and genotyping accuracy across all samples. The development of the GVCF workflow has made this process scalable to tens of thousands of individuals, making it ideal for population genetics, disease association studies, and projects where adding samples incrementally is a common requirement.The DeepVariant pipeline offers a compelling alternative, characterized by exceptional single-sample accuracy and a streamlined workflow. By leveraging a deep learning model, it obviates the need for Base Quality Score Recalibration and complex statistical filtering (VQSR), reducing both the number of steps and the number of tunable parameters. This makes it an excellent choice for smaller-scale studies, clinical applications where rapid turnaround of single samples is paramount, or research focused on complex indel variants where DeepVariant has shown a performance advantage.Ultimately, both pipelines, when implemented within a robust framework like Snakemake, provide a powerful and reproducible means of translating raw sequencing data into biological insights. The selection of the appropriate tool is a critical experimental design choice that depends on a careful consideration of the trade-offs between cohort-level statistical power and single-sample simplicity and accuracy.Table 2: GATK vs. DeepVariant Pipeline Feature ComparisonFeatureGATK Best Practices PipelineDeepVariant PipelineCalling AlgorithmLocal de-novo assembly of haplotypes and statistical modeling (HaplotypeCaller)Deep convolutional neural network (CNN) for image-based classificationRequired Pre-processingAlignment, Duplicate Marking, Base Quality Score Recalibration (BQSR)Alignment, Duplicate Marking (BQSR is not required)Post-calling FilteringTypically requires Variant Quality Score Recalibration (VQSR) or hard filteringSimple QUAL score or FILTER status thresholding is sufficientScalability ModelPer-sample gVCF generation followed by cohort joint genotyping (GenomicsDBImport + GenotypeGVCFs)Per-sample gVCF generation followed by cohort merging (e.g., GLnexus)Key StrengthHigh sensitivity and accuracy in large cohorts due to joint genotyping; extensive toolset for data manipulationHighest single-sample accuracy, especially for indels; simplified workflow with fewer steps and parametersKey ConsiderationMore complex workflow with more steps (BQSR, VQSR); VQSR requires a sufficiently large cohort to train its modelJoint genotyping relies on a third-party tool (GLnexus); model performance may vary for non-human organisms